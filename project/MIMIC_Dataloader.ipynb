{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZpQ2R-oG1FWB"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/mimic_dataset\" # Path to \"mimic_dataset\" folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFfLNsCenzLh",
        "outputId": "ff068a0f-dac0-460b-a047-3011ff99ee99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gf4nvPxwthD0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import *\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9XeP8iWNglgV"
      },
      "outputs": [],
      "source": [
        "# config class for setting hyperparameters\n",
        "class cfg:\n",
        "    IMG_SIZE = 224\n",
        "    BATCH = 64\n",
        "    EPOCHS = 100\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qgOUG0sTE_yV",
        "outputId": "c71e21cc-3265-47e7-c221-4c0e76095bb3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2af479f8-c175-4856-8617-6416fb6725ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dicom</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>icd_code</th>\n",
              "      <th>edema</th>\n",
              "      <th>viewposition</th>\n",
              "      <th>study_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>ventilation_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54affd39-8bf24209-232bac8a-df6c277a-398ee8a5</td>\n",
              "      <td>10000980</td>\n",
              "      <td>I5023</td>\n",
              "      <td>1.0</td>\n",
              "      <td>AP</td>\n",
              "      <td>58206436</td>\n",
              "      <td>25911675</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6ad819bb-bae74eb9-7b663e90-b8deabd7-57f8054a</td>\n",
              "      <td>10000980</td>\n",
              "      <td>I5023</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PA</td>\n",
              "      <td>54935705</td>\n",
              "      <td>29659838</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>051b7911-cb00aec9-0b309188-89803662-303ec278</td>\n",
              "      <td>10002131</td>\n",
              "      <td>I5033</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AP</td>\n",
              "      <td>52823782</td>\n",
              "      <td>24065018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4873aa08-977bfd31-fb492e64-6ef432d1-3f12cbe3</td>\n",
              "      <td>10002430</td>\n",
              "      <td>I5033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PA</td>\n",
              "      <td>53254222</td>\n",
              "      <td>24513842</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e0275ad1-1e6a7451-c3960f5f-1267a188-547b73a1</td>\n",
              "      <td>10003502</td>\n",
              "      <td>I5033</td>\n",
              "      <td>1.0</td>\n",
              "      <td>AP</td>\n",
              "      <td>52309364</td>\n",
              "      <td>29011269</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2af479f8-c175-4856-8617-6416fb6725ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2af479f8-c175-4856-8617-6416fb6725ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2af479f8-c175-4856-8617-6416fb6725ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          dicom  subject_id icd_code  edema  \\\n",
              "0  54affd39-8bf24209-232bac8a-df6c277a-398ee8a5    10000980    I5023    1.0   \n",
              "1  6ad819bb-bae74eb9-7b663e90-b8deabd7-57f8054a    10000980    I5023    1.0   \n",
              "2  051b7911-cb00aec9-0b309188-89803662-303ec278    10002131    I5033    NaN   \n",
              "3  4873aa08-977bfd31-fb492e64-6ef432d1-3f12cbe3    10002430    I5033    0.0   \n",
              "4  e0275ad1-1e6a7451-c3960f5f-1267a188-547b73a1    10003502    I5033    1.0   \n",
              "\n",
              "  viewposition  study_id   hadm_id ventilation_status  \n",
              "0           AP  58206436  25911675                NaN  \n",
              "1           PA  54935705  29659838                NaN  \n",
              "2           AP  52823782  24065018                NaN  \n",
              "3           PA  53254222  24513842                NaN  \n",
              "4           AP  52309364  29011269                NaN  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_dir = os.path.join(data_dir, 'mimic_icd10_labels.csv')\n",
        "labels = pd.read_csv(labels_dir)\n",
        "# Only keep xrays with PA or AP views\n",
        "labels = labels[labels['viewposition'].isin(['PA', 'AP'])].drop_duplicates(subset=['dicom']).reset_index(drop=True)\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gehA5nj7lrrR"
      },
      "outputs": [],
      "source": [
        "# change image names to be absolute paths of all images\n",
        "def format_name(row, data_dir):\n",
        "  '''Takes one row in the dataframe and returns the file location for the particular image'''\n",
        "  img_dir = os.path.join(data_dir, 'data')\n",
        "  filepath = os.path.join(img_dir, row['dicom']) + '.jpg'\n",
        "  return filepath\n",
        "\n",
        "def binarize_labels(row):\n",
        "  # 0 is reduced ejection fraction, 1 is preserved ejection fraction\n",
        "  return 0 if row['icd_code'].startswith('I502') else 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqiivn9KTPU0",
        "outputId": "f351fc94-ff46-4e7b-c72c-dfb79272414f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (2354,)\n",
            "y_train shape: (2354,)\n",
            "X_val shape: (262,)\n",
            "y_val shape: (262,)\n",
            "X_test shape: (872,)\n",
            "y_test shape: (872,)\n"
          ]
        }
      ],
      "source": [
        "# Create column containing paths to each image\n",
        "labels['img_path'] = labels.apply(lambda row: format_name(row, data_dir=data_dir), axis=1)\n",
        "# Create column with binary label for each image\n",
        "labels['binary_label'] = labels.apply(lambda row: binarize_labels(row), axis=1)\n",
        "# For experimenting with edema only patients\n",
        "xrays_with_edema = labels[labels['edema']==1]\n",
        "\n",
        "# Train test split\n",
        "paths, img_labels = labels['img_path'].values, labels['binary_label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(paths, img_labels, test_size=.25, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "print(f'X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\\nX_val shape: {X_val.shape}\\ny_val shape: {y_val.shape}\\nX_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}')\n",
        "\n",
        "# Train test split for edema only patients\n",
        "edema_paths, edema_img_labels = xrays_with_edema['img_path'].values, xrays_with_edema['binary_label'].values\n",
        "X_train_edema, X_test_edema, y_train_edema, y_test_edema = train_test_split(edema_paths, edema_img_labels, test_size=.25, random_state=42)\n",
        "X_train_edema, X_val_edema, y_train_edema, y_val_edema = train_test_split(X_train_edema, y_train_edema, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8nN4BfWJpHZt"
      },
      "outputs": [],
      "source": [
        "class EdemaCxrDataset(Dataset):\n",
        "    \"\"\"Mimic Edema CXR Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, img_paths, labels):\n",
        "        '''Args:\n",
        "            paths: array of image paths\n",
        "            labels: array of labels for each image in img_paths\n",
        "        '''\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load image from google drive into pytorch tensor\n",
        "        file_path = self.img_paths[idx]\n",
        "        image = read_image(file_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = TF.resize(image, (cfg.IMG_SIZE, cfg.IMG_SIZE)) # reshape to (1, 224, 224)\n",
        "        image = image.repeat(3,1,1)         # tile to (3, 224, 224)\n",
        "        image = image / 255 # normalize between 0 and 1\n",
        "        # TODO: consider adding more data augmentations like:\n",
        "        # https://github.com/MLforHealth/CXR_Fairness/blob/c2a0e884171d6418e28d59dca1ccfb80a3f125fe/cxr_fairness/data/data.py#L33\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OW3jE7qEw4mZ"
      },
      "outputs": [],
      "source": [
        "# Create datasets and dataloaders\n",
        "train_dataset = EdemaCxrDataset(X_train, y_train)\n",
        "val_dataset = EdemaCxrDataset(X_val, y_val)\n",
        "test_dataset = EdemaCxrDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH)\n",
        "test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH)\n",
        "\n",
        "# Loaders for edema only patients\n",
        "edema_train_dataset = EdemaCxrDataset(X_train_edema, y_train_edema)\n",
        "edema_val_dataset = EdemaCxrDataset(X_val_edema, y_val_edema)\n",
        "edema_test_dataset = EdemaCxrDataset(X_test_edema, y_test_edema)\n",
        "edema_train_loader = DataLoader(edema_train_dataset, batch_size=cfg.BATCH, shuffle=True)\n",
        "edema_val_loader = DataLoader(edema_val_dataset, batch_size=cfg.BATCH)\n",
        "edema_test_loader = DataLoader(edema_test_dataset, batch_size=cfg.BATCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byVYw0wPgcKM"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1e7620539d504e098e11ab72fc903d8d",
            "3430780679f94f7984fc0198f5276895",
            "538a0b1f3bd948e1a3dbed60805566b8",
            "e8ac580798d546e4ae62692b3daf3940",
            "91c11653af494e8c9c118d6d4b88cb1f",
            "7c9e93ef72a04ab4bb2d8695fa674ea9",
            "97f1a70a8d164cee97f75bf09cbf5d90",
            "69ecb786bb0045eaa4c3c51d72f40455",
            "0468cc0c43fe44c484e9a42a4ed52633",
            "8eafd5b665a247f680928ac229cf2a7c",
            "061d9a2b627241b2baf42f57cf65355b"
          ]
        },
        "id": "fL5flifuh_L2",
        "outputId": "b593c294-d290-4d6f-e5fb-539d4f99546a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e7620539d504e098e11ab72fc903d8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import resnet50 model\n",
        "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(in_features=2048, out_features=1, bias=True)\n",
        "model.to(cfg.device)\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# loss function \n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wUef0E5z6k2G"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "  '''custom implementation of early stopping. Once the patience and min_delta threshold have been met it will stop training'''\n",
        "\n",
        "  def __init__(self, patience=1, min_delta=0):\n",
        "      self.patience = patience\n",
        "      self.min_delta = min_delta\n",
        "      self.counter = 0\n",
        "      self.min_validation_loss = np.inf\n",
        "\n",
        "  def early_stop(self, validation_loss):\n",
        "      if validation_loss < self.min_validation_loss:\n",
        "          self.min_validation_loss = validation_loss\n",
        "          self.counter = 0\n",
        "      elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "          self.counter += 1\n",
        "          if self.counter >= self.patience:\n",
        "              return True\n",
        "      return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PPfwmi813XH6"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, return_dict=False, get_auc=False):\n",
        "  '''\n",
        "    evaluates model on a given dataset (primarily used to get metrics on validation and test datasets)\n",
        "    Args: model: imported torch model\n",
        "          dataloader: pytorch dataloader object\n",
        "          criterion: loss function\n",
        "          return_dict: whether to return metrics as a dictionary or string\n",
        "          get_auc: whether to compute and return the auc as well\n",
        "          '''\n",
        "  \n",
        "  model.eval()\n",
        "  list_of_labels = []\n",
        "  # Collection of all model predictions\n",
        "  all_predictions = torch.Tensor()\n",
        "  loss = 0.0\n",
        "  \n",
        "  # Generate predictions\n",
        "  for i, (imgs, labels) in enumerate(dataloader):\n",
        "      imgs, labels = imgs.float().to(cfg.device), labels.to(cfg.device).float()\n",
        "      list_of_labels.extend(labels.cpu().numpy())\n",
        "      with torch.no_grad():\n",
        "          output = nn.Sigmoid()(model(imgs)).squeeze(1)\n",
        "          loss += float(criterion(output, labels)) / len(imgs)\n",
        "      \n",
        "      # Predictions for current batch of data \n",
        "      preds = output.cpu().detach().apply_(lambda x: 1.0 if x > 0.5 else 0.0)\n",
        "      all_predictions = torch.cat([all_predictions.to(cfg.device), preds.to(cfg.device)], dim=0)\n",
        "  \n",
        "  # Generate metrics\n",
        "  metrics = classification_report(np.array(list_of_labels), all_predictions.cpu().numpy(), \n",
        "                                  target_names=['Reduced Ejection Fraction', 'Preserved Ejection Fraction'],\n",
        "                                  output_dict=return_dict)\n",
        "  if get_auc:\n",
        "    auc = roc_auc_score(np.array(list_of_labels), all_predictions.cpu().numpy())\n",
        "    return loss, metrics, auc\n",
        "\n",
        "  return loss, metrics\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "def fit(model, train_dataloader, validation_dataloader, optimizer, criterion, epochs, save_path='mimic_model_weights.pt', earlystopper=None):\n",
        "  ''' trains a model on data from a dataloader\n",
        "      Args: model: imported torch model\n",
        "            train_dataloader: pytorch dataloader object for training set\n",
        "            validation_dataloader: pytorch dataloader object for validation set\n",
        "            optimizer: pytorch optimizer object\n",
        "            criterion: loss function\n",
        "            epochs: number of epochs to train for (this option is set in the config (cfg) class)\n",
        "  '''\n",
        "\n",
        "  min_val_loss = np.inf # for model checkpoints\n",
        "  for epoch in range(epochs):\n",
        "    with tqdm(train_dataloader, unit='batch', position=0, leave=True) as tepoch:\n",
        "      for imgs, labels in tepoch:\n",
        "        tepoch.set_description(f'Epoch {epoch+1}')\n",
        "\n",
        "        model.train()\n",
        "        imgs, labels = imgs.to(cfg.device), labels.to(cfg.device).float()\n",
        "        # Forward pass\n",
        "        output = nn.Sigmoid()(model(imgs.float())).squeeze(1)\n",
        "        # Compute loss for one batch\n",
        "        loss = criterion(output, labels)\n",
        "        # Zero out old gradients held by optimizer so we can compute new gradients for this current batch\n",
        "        optimizer.zero_grad()\n",
        "        # Calculate new gradient of loss wrt current model weights\n",
        "        loss.backward()\n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "        tepoch.set_postfix(training_loss=(float(loss)))\n",
        "      \n",
        "      # Generate metrics on validation dataset\n",
        "      validation_loss, validation_metrics = evaluate(model=model, dataloader=validation_dataloader, criterion=criterion)\n",
        "      tepoch.set_postfix(loss=validation_loss)\n",
        "      scheduler.step(validation_loss)\n",
        "      print(f'Validation Loss: {validation_loss}\\n-------------------------------------------------------\\n{validation_metrics}\\n\\n')\n",
        "      \n",
        "      # saving checkpoint\n",
        "      if validation_loss < min_val_loss:\n",
        "        print(f'[CHECKPOINT] New best validation loss achieved. Old best was {min_val_loss}, new_best is {validation_loss}. Saving weights to {data_dir}/{save_path}...')\n",
        "        torch.save(model.state_dict(),  os.path.join(data_dir, save_path))\n",
        "        min_val_loss = float(validation_loss)   \n",
        "\n",
        "      # Check to make sure earlystopping criteria not met\n",
        "      if earlystopper is not None:\n",
        "        if earlystopper.early_stop(validation_loss):\n",
        "          # stop training and save model weights\n",
        "          print(f'[EARLYSTOPPING]\\n stopped training. Final validation loss was {validation_loss}\\nSaving weights to {data_dir}/{save_path}...')\n",
        "          torch.save(model.state_dict(), os.path.join(data_dir, save_path))\n",
        "          return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWTXySKJkkqW",
        "outputId": "cbbaa12f-b8f2-4664-f97b-c1a47a3661cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  11%|█         | 4/37 [03:48<29:55, 54.40s/batch, training_loss=0.646]"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "early_stopping = EarlyStopper(patience=10, min_delta=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=3, threshold=1e-3, verbose=True)\n",
        "fit(model=model, train_dataloader=train_loader, validation_dataloader=val_loader, \n",
        "    optimizer=optimizer, criterion=criterion, epochs=cfg.EPOCHS, earlystopper=early_stopping, save_path='resnet50_weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqtzoSgtU4sq"
      },
      "outputs": [],
      "source": [
        "# # Load in pre-trained model weights from edema dataset and fine tune on our dataset\n",
        "# state_dic = torch.load('/content/drive/MyDrive/mimic_dataset/edema_mimic_model_weights.pt', map_location=torch.device('cpu'))['model_state_dict']\n",
        "# model.load_state_dict(state_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh0-w7faIthV"
      },
      "outputs": [],
      "source": [
        "# early_stopping = EarlyStopper(patience=5, min_delta=0.1)\n",
        "# fit(model=model, train_dataloader=edema_train_loader, validation_dataloader=edema_val_loader, \n",
        "#     optimizer=optimizer, criterion=criterion, epochs=cfg.EPOCHS, earlystopper=early_stopping, save_path='edema_pretrained_model_dic.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdnlJ-MboOdE"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10,10))\n",
        "# ref_prec, ref_rec, ref_f1 = test_metrics['Reduced Ejection Fraction']['precision'], test_metrics['Reduced Ejection Fraction']['recall'], test_metrics['Reduced Ejection Fraction']['f1-score']\n",
        "# pef_prec, pef_rec, pef_f1 = test_metrics['Preserved Ejection Fraction']['precision'], test_metrics['Preserved Ejection Fraction']['recall'], test_metrics['Preserved Ejection Fraction']['f1-score']\n",
        "# X = np.arange(3)\n",
        "# plt.bar(X, [ref_prec, ref_rec,ref_f1], width=0.2, label='Reduced Ejection Fraction')\n",
        "# plt.bar(X+0.2, [pef_prec, pef_rec, pef_f1], width=0.2, label='Preserved Ejection Fraction')\n",
        "# plt.xticks(range(3), list(test_metrics['Reduced Ejection Fraction'].keys()))\n",
        "# plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQoRWwbx01jp"
      },
      "outputs": [],
      "source": [
        "# loss, printable_metrics, auc = evaluate(model=model, dataloader=test_loader, criterion=criterion, return_dict=False, get_auc=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9izY-HwR1YVx"
      },
      "outputs": [],
      "source": [
        "# print(printable_metrics, f'auc={auc:.4f}', sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4HFYIqn3NZX"
      },
      "outputs": [],
      "source": [
        "test_loss, test_metrics, test_auc = evaluate(model=model, dataloader=test_loader, criterion=criterion, return_dict=False, get_auc=True)\n",
        "print(test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijgCzFa4r2m7"
      },
      "outputs": [],
      "source": [
        "# Error Analysis\n",
        "# Get predictions on test set\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(cfg.device), labels.to(cfg.device).float()\n",
        "        output = nn.Sigmoid()(model(imgs.float())).squeeze(1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(output.cpu().numpy())\n",
        "\n",
        "# Get indices of misclassified images\n",
        "misclassified = np.where(np.array(y_true) != np.array(y_pred).round())[0]\n",
        "print(f'Number of misclassified images: {len(misclassified)}')\n",
        "\n",
        "# Get indices of correctly classified images\n",
        "correctly_classified = np.where(np.array(y_true) == np.array(y_pred).round())[0]\n",
        "print(f'Number of correctly classified images: {len(correctly_classified)}')\n",
        "\n",
        "# Get indices of images with high confidence\n",
        "high_confidence = np.where(np.array(y_pred) > 0.9)[0]\n",
        "print(f'Number of images with high confidence: {len(high_confidence)}')\n",
        "\n",
        "# Get indices of images with low confidence\n",
        "low_confidence = np.where(np.array(y_pred) < 0.1)[0]\n",
        "print(f'Number of images with low confidence: {len(low_confidence)}')\n",
        "\n",
        "# Get indices of images with high confidence and misclassified\n",
        "high_confidence_misclassified = np.intersect1d(misclassified, high_confidence)\n",
        "print(f'Number of images with high confidence and misclassified: {len(high_confidence_misclassified)}')\n",
        "\n",
        "# Get indices of images with low confidence and misclassified\n",
        "low_confidence_misclassified = np.intersect1d(misclassified, low_confidence)\n",
        "print(f'Number of images with low confidence and misclassified: {len(low_confidence_misclassified)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'misclassified' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/19/s4zdyd_16mb8h239mgwds83m0000gn/T/ipykernel_21351/2545999309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Get saliency maps for misclassified images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmisclassified_saliency_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmisclassified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'misclassified' is not defined"
          ]
        }
      ],
      "source": [
        "# saliency map\n",
        "def get_saliency_map(img, label, model):\n",
        "    model.eval()\n",
        "    img.requires_grad = True\n",
        "    output = model(img)\n",
        "    output = nn.Sigmoid()(output).squeeze(1)\n",
        "    #label = torch.tensor(label).to(cfg.device)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    saliency_map = img.grad.abs().squeeze(0).cpu().numpy()\n",
        "    return saliency_map\n",
        "\n",
        "\n",
        "\n",
        "# Get saliency maps for misclassified images\n",
        "misclassified_saliency_maps = []\n",
        "for idx in misclassified:\n",
        "    img, label = test_dataset[idx]\n",
        "    img = img.unsqueeze(0).to(cfg.device)\n",
        "    label = torch.tensor(label).unsqueeze(0).to(cfg.device)\n",
        "    label = label.float()\n",
        "    saliency_map = get_saliency_map(img, label, model)\n",
        "\n",
        "    misclassified_saliency_maps.append(saliency_map)\n",
        "\n",
        "# plot saliency maps\n",
        "def plot_saliency_maps(saliency_maps, images, labels, preds, figsize=(20, 20)):\n",
        "    n = len(saliency_maps)\n",
        "    fig, axes = plt.subplots(nrows=n, ncols=3, figsize=figsize)\n",
        "    for i in range(n):\n",
        "        axes[i, 0].imshow(images[i].transpose((1, 2, 0), cmap='jet'))\n",
        "        axes[i, 0].set_title(f'Label: {labels[i]}')\n",
        "        axes[i, 1].imshow(saliency_maps[i], cmap='jet')\n",
        "        axes[i, 1].set_title(f'Prediction: {preds[i]}')\n",
        "        axes[i, 2].imshow(images[i].transpose((1, 2, 0)))\n",
        "        axes[i, 2].imshow(saliency_maps[i], cmap='jet', alpha=0.5)\n",
        "        axes[i, 2].set_title(f'Overlay')\n",
        "    #plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# plot saliency maps for misclassified images\n",
        "#TypeError: transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n",
        "# * (int dim0, int dim1)\n",
        "# * (name dim0, name dim1)\n",
        "misclassified_images = [test_dataset[idx][0].numpy() for idx in misclassified]\n",
        "misclassified_labels = [test_dataset[idx][1] for idx in misclassified]\n",
        "misclassified_preds = [np.round(np.array(y_pred)[idx], 2) for idx in misclassified]\n",
        "# plot 6 misclassified images\n",
        "plot_saliency_maps(misclassified_saliency_maps, misclassified_images, misclassified_labels, misclassified_preds)\n",
        "\n",
        "# Get saliency maps for correctly classified images\n",
        "correctly_classified_saliency_maps = []\n",
        "for idx in correctly_classified:\n",
        "    img, label = test_dataset[idx]\n",
        "    img = img.unsqueeze(0).to(cfg.device)\n",
        "    label = torch.tensor(label).unsqueeze(0).to(cfg.device)\n",
        "    label = label.float()\n",
        "    saliency_map = get_saliency_map(img, label, model)\n",
        "\n",
        "    correctly_classified_saliency_maps.append(saliency_map)\n",
        "\n",
        "# plot saliency maps for correctly classified images\n",
        "correctly_classified_images = [test_dataset[idx][0].numpy() for idx in correctly_classified]\n",
        "correctly_classified_labels = [test_dataset[idx][1] for idx in correctly_classified]\n",
        "correctly_classified_preds = [np.round(np.array(y_pred)[idx], 2) for idx in correctly_classified]\n",
        "# plot 6 correctly classified images\n",
        "plot_saliency_maps(correctly_classified_saliency_maps[:6], correctly_classified_images[:6], correctly_classified_labels[:6], correctly_classified_preds[:6])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get grad cam for misclassified images\n",
        "def get_grad_cam(img, label, model):\n",
        "    model.eval()\n",
        "    img.requires_grad = True\n",
        "    output = model(img)\n",
        "    output = nn.Sigmoid()(output).squeeze(1)\n",
        "    #label = torch.tensor(label).to(cfg.device)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    grad_cam = img.grad.abs().squeeze(0).cpu().numpy()\n",
        "    return grad_cam\n",
        "\n",
        "misclassified_grad_cam = []\n",
        "for idx in misclassified:\n",
        "    img, label = test_dataset[idx]\n",
        "    img = img.unsqueeze(0).to(cfg.device)\n",
        "    label = torch.tensor(label).unsqueeze(0).to(cfg.device)\n",
        "    label = label.float()\n",
        "    grad_cam = get_grad_cam(img, label, model)\n",
        "\n",
        "    misclassified_grad_cam.append(grad_cam)\n",
        "\n",
        "# write function to plot grad cam\n",
        "def plot_grad_cam(grad_cam, images, labels, preds, figsize=(20, 20)):\n",
        "    n = len(grad_cam)\n",
        "    fig, axes = plt.subplots(nrows=n, ncols=3, figsize=figsize)\n",
        "    for i in range(n):\n",
        "        axes[i, 0].imshow(images[i].transpose((1, 2, 0), cmap='\n",
        "\n",
        "# plot grad cam for misclassified images\n",
        "plot_saliency_maps(misclassified_grad_cam[:6], misclassified_images[:6], misclassified_labels[:6], misclassified_preds[:6])\n",
        "# Get grad cam for correctly classified images\n",
        "correctly_classified_grad_cam = []\n",
        "for idx in correctly_classified:\n",
        "    img, label = test_dataset[idx]\n",
        "    img = img.unsqueeze(0).to(cfg.device)\n",
        "    label = torch.tensor(label).unsqueeze(0).to(cfg.device)\n",
        "    label = label.float()\n",
        "    grad_cam = get_grad_cam(img, label, model)\n",
        "\n",
        "    correctly_classified_grad_cam.append(grad_cam)\n",
        "\n",
        "# plot grad cam for correctly classified images\n",
        "plot_saliency_maps(correctly_classified_grad_cam[:6], correctly_classified_images[:6], correctly_classified_labels[:6], correctly_classified_preds[:6])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('fsvi')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e302dde289e78300c263ede8c67906e387b93cade991be4f70998b8d7518f0c"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0468cc0c43fe44c484e9a42a4ed52633": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "061d9a2b627241b2baf42f57cf65355b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7620539d504e098e11ab72fc903d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3430780679f94f7984fc0198f5276895",
              "IPY_MODEL_538a0b1f3bd948e1a3dbed60805566b8",
              "IPY_MODEL_e8ac580798d546e4ae62692b3daf3940"
            ],
            "layout": "IPY_MODEL_91c11653af494e8c9c118d6d4b88cb1f"
          }
        },
        "3430780679f94f7984fc0198f5276895": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c9e93ef72a04ab4bb2d8695fa674ea9",
            "placeholder": "​",
            "style": "IPY_MODEL_97f1a70a8d164cee97f75bf09cbf5d90",
            "value": "100%"
          }
        },
        "538a0b1f3bd948e1a3dbed60805566b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69ecb786bb0045eaa4c3c51d72f40455",
            "max": 102540417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0468cc0c43fe44c484e9a42a4ed52633",
            "value": 102540417
          }
        },
        "69ecb786bb0045eaa4c3c51d72f40455": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9e93ef72a04ab4bb2d8695fa674ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eafd5b665a247f680928ac229cf2a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91c11653af494e8c9c118d6d4b88cb1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f1a70a8d164cee97f75bf09cbf5d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8ac580798d546e4ae62692b3daf3940": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eafd5b665a247f680928ac229cf2a7c",
            "placeholder": "​",
            "style": "IPY_MODEL_061d9a2b627241b2baf42f57cf65355b",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 275MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
