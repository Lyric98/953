{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U93wKjNTK5ET"
      },
      "source": [
        "# Problem 1: Modeling with ClinicalBERT Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gv0j1MLfBrN"
      },
      "source": [
        "Note: to avoid consuming the GPU resources allocated to you by colab on the parts you don't need a GPU for, make sure you use a CPU runtime (Runtime > Change Runtime Type > Hardware accelerator: None) until the notebook indicates otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUmfHV_VK9g9"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tv3lgtR4nEsM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (4.23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.20.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
            "Requirement already satisfied: requests in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.20.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\n",
            "Collecting umap-learn\n",
            "  Using cached umap-learn-0.5.3.tar.gz (88 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.20.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.24.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.6.2)\n",
            "Requirement already satisfied: numba>=0.49 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.53.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.5.4)\n",
            "Requirement already satisfied: tqdm in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (4.59.0)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (0.36.0)\n",
            "Requirement already satisfied: setuptools in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (52.0.0.post20210125)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/liyanran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n",
            "Building wheels for collected packages: umap-learn\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82820 sha256=9ac9a369da199dc7bf90cd848430a5498387b7ec9f1d3d2b086c8a5dc7bb5879\n",
            "  Stored in directory: /Users/liyanran/Library/Caches/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "Successfully built umap-learn\n",
            "Installing collected packages: umap-learn\n",
            "Successfully installed umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install scikit-learn\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfVoxO-7BBuA"
      },
      "source": [
        "## Setting up Google Drive\n",
        "Copy the data at the [following link](https://drive.google.com/drive/folders/1G5NuAnUSaKzcry-tzgPZKxafG_vcOzX9?usp=sharing) to a folder in your own drive and set the path to that folder below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNe_F-2HBAdB"
      },
      "outputs": [],
      "source": [
        "# Path to saved data\n",
        "#------YOUR CODE HERE--------\n",
        "data_path = \"/content/drive/MyDrive/path_to_your_folder\"\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9j60f3KrfsG"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, drive\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7qfVkr1MzXgu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os \n",
        "import random\n",
        "import sklearn\n",
        "import importlib\n",
        "import pickle\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.utils import data\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, BertConfig, BertTokenizer, BertForMaskedLM, InputExample\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# Add random seed\n",
        "random.seed(456)\n",
        "np.random.seed(456)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLNCV2jvqcpP"
      },
      "source": [
        "(a) You can read more about ClinicalBERT [here](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sqZxtqxpMmg_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n",
        "# Initialize the model\n",
        "model = BertForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\") \n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "J1v0gkPstnaG"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEjK57qaLCV7"
      },
      "source": [
        "(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "k9RM0cJxTVTe"
      },
      "outputs": [],
      "source": [
        "def fill_blank(text: str, model: BertForMaskedLM, tokenizer: BertTokenizer) -> str:\n",
        "    '''\n",
        "    Given a sentence with a single blank (denoted by an underscore), queries the BERT model to \n",
        "        fill in the missing token.\n",
        "        \n",
        "    Inputs:\n",
        "        - text: sentence containing a single underscore corresponding to the missing token\n",
        "        - model: pytorch ClinicalBERT model, of type BertForMaskedLM\n",
        "        - tokenizer: BertTokenizer object\n",
        "    \n",
        "    Output:\n",
        "        - string corresponding to the sentence where the underscore is replaced with the most likely token\n",
        "    '''\n",
        "    random.seed(456)\n",
        "    np.random.seed(456)\n",
        "    torch.manual_seed(456)\n",
        "    \n",
        "    #------YOUR CODE HERE--------\n",
        "    # Replace the underscore by [MASK] and store the result in masked_str\n",
        "    masked_str = text.replace(\"_\", \"[MASK]\") \n",
        "\n",
        "    # Tokenize the masked string and store the tokens in inputs\n",
        "    inputs = tokenizer(masked_str, return_tensors=\"pt\")\n",
        "    #------YOUR CODE ENDS--------\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    with torch.no_grad():\n",
        "        #------YOUR CODE HERE--------\n",
        "        # Compute the logits (log probabilities) from the model\n",
        "        logits =  model(**inputs)[0]     \n",
        "        #------YOUR CODE ENDS--------\n",
        "\n",
        "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "    predicted_logits = logits[0, mask_token_index] \n",
        "    #------YOUR CODE HERE--------\n",
        "    # Select the most likely token in predicted_logits\n",
        "    predicted_token_id = torch.argmax(predicted_logits).item()\n",
        "\n",
        "    # Use the tokenizer to decode the token id into a string\n",
        "    pred = tokenizer.decode([predicted_token_id]) \n",
        "    #------YOUR CODE ENDS--------\n",
        "\n",
        "    return text.replace('_', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hwadavOxsn7v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sentence: 30 yo white female helping other nurses at the ICU\n",
            "Expected sentence: 30 yo white female helping other nurses at the ICU\n",
            "Predicted sentence: 30 yo white male helping other doctors at the ICU\n",
            "Expected sentence: 30 yo white male helping other doctors at the ICU\n"
          ]
        }
      ],
      "source": [
        "# Test fill_blank\n",
        "nurse_sent = '30 yo white _ helping other nurses at the ICU'\n",
        "doc_sent = '30 yo white _ helping other doctors at the ICU'\n",
        "print(f\"Predicted sentence: {fill_blank(nurse_sent, model, tokenizer)}\")\n",
        "print(\"Expected sentence: 30 yo white female helping other nurses at the ICU\")\n",
        "print(f\"Predicted sentence: {fill_blank(doc_sent, model, tokenizer)}\")\n",
        "print(\"Expected sentence: 30 yo white male helping other doctors at the ICU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'30 yo white male helping other doctors at the ICU'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fill_blank(doc_sent, model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lReDVacVwAPU"
      },
      "source": [
        "(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "yxnT3yzFwSFL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1 (completed): The renal dysfunction score changes over time among the black African population.\n",
            "Sentence 2 (completed): The cardiovascular dysfunction score changes over time among the elderly women population.\n"
          ]
        }
      ],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "sent1 = 'The renal dysfunction score changes over time among the black _ population.'\n",
        "sent2 = 'The cardiovascular dysfunction score changes over time among the _ women population.'\n",
        "#------YOUR CODE ENDS--------\n",
        "print(f\"Sentence 1 (completed): {fill_blank(sent1, model, tokenizer)}\")\n",
        "print(f\"Sentence 2 (completed): {fill_blank(sent2, model, tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XX3lVy3xDAm"
      },
      "source": [
        "(d) Answer in your report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bcW_vqkxfYR"
      },
      "source": [
        "(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p87qzrBG1wOH"
      },
      "source": [
        "Change the runtime to GPU for this part (Runtime > Change Runtime Type > Hardware Accelerator: GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OPLJobKBp1n3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-756c430c9f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text_and_hypertension_data.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data_path' is not defined"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "df = pd.read_hdf(os.path.join(data_path, \"text_and_hypertension_data.h5\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tvYSo6JQtZW3"
      },
      "outputs": [],
      "source": [
        "def get_sent_rep(model, tokenizer, txt):\n",
        "    \"\"\"\n",
        "    Compute the sentence representation and return it as a numpy array\n",
        "    If done correctly, the numpy array should be of size 768\n",
        "    \"\"\"\n",
        "    # Tokenize the input txt and store the result in inputs\n",
        "    # Remember to set truncation=True and max_length=512\n",
        "    #------YOUR CODE HERE--------\n",
        "    inputs = tokenizer(txt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    #------YOUR CODE ENDS--------\n",
        "    inputs.to(device)\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        #------YOUR CODE HERE--------\n",
        "        # Compute the model outputs and store the result in outputs\n",
        "        # Make sure output_hidden_states=True\n",
        "        outputs = model(**inputs, output_hidden_states=True) \n",
        "        #------YOUR CODE ENDS--------\n",
        "    \n",
        "        embed = outputs.hidden_states[-1]\n",
        "\n",
        "        #------YOUR CODE HERE--------\n",
        "        # embed (of size [1, input_length, 768])\n",
        "        # contains the hidden states corresponding to each\n",
        "        # token at the final layer of the model\n",
        "        # Each hidden state is a vector of size 768\n",
        "        # Compute the mean of these vectors to get a representation\n",
        "        # of the input sentence, and store the mean again in embed\n",
        "        embed = torch.mean(embed, dim=1)\n",
        "        #------YOUR CODE ENDS--------\n",
        "\n",
        "        embed = embed.squeeze()\n",
        "\n",
        "    return embed.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eteA2T5q5D89"
      },
      "source": [
        "The following block of code should take around 15min to run on a GPU the first time it is run. Make sure to save its result in your drive to avoid needing to run it again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "U-wPQFJxPKAs"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-ad197877fd63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecompute_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Only regenerate embeds if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embeds.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrecompute_embeds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_path' is not defined"
          ]
        }
      ],
      "source": [
        "recompute_embeds = False\n",
        "# Only regenerate embeds if necessary\n",
        "if not os.path.exists(os.path.join(data_path, \"embeds.npy\")) or recompute_embeds == True:\n",
        "    # Generate embeddings\n",
        "    num_pts = len(df)\n",
        "    embeds = [None]* num_pts\n",
        "    start = time.time()\n",
        "    for row_idx in range(num_pts):\n",
        "        note_data = df.iloc[row_idx][\"text\"]\n",
        "        embeds[row_idx] = get_sent_rep(model, tokenizer, note_data)\n",
        "    print(time.time() - start)\n",
        "    X = np.stack(embeds, axis=0)\n",
        "    with open(os.path.join(data_path, \"embeds.npy\"), \"wb\") as f:\n",
        "        np.save(f, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhqQP6_O6U8s"
      },
      "source": [
        "(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwoanXGrHXNz"
      },
      "source": [
        "You can change the runtime back to CPU to avoid using your GPU allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jvUc6PJ_dCY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get train and test data\n",
        "with open(os.path.join(data_path, \"embeds.npy\"), \"rb\") as f:\n",
        "    # X contains one embedding per row corresponding to\n",
        "    # the discharge summary of the patient in that row\n",
        "    # in the dataset\n",
        "    X = np.load(f)\n",
        "\n",
        "# y contains whether the patient in a particular row had\n",
        "# hypertension during their ICU stay\n",
        "y = df['Hypertension'].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=456)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U95_xwRXlMUo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Scale the train data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcf-lDpoYnTJ"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Create and fit your logistic regression model on the training data\n",
        "# Make sure to use multi_class = \"multinomial\" and class_weight=\"balanced\"\n",
        "\n",
        "LR_model = LogisticRegression(multi_class = \"multinomial\", class_weight=\"balanced\")\n",
        "LR_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "#------YOUR CODE ENDS--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmjRSWlq9oRw"
      },
      "source": [
        "(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgEyht5id7sI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "#------YOUR CODE HERE--------\n",
        "# Compute the performance metrics on the training set\n",
        "#Compute the accuracy\n",
        "train_acc = accuracy_score(y_train, LR_model.predict(X_train_scaled))\n",
        "#test_acc = accuracy_score(y_test, LR_model.predict(scaler.transform(X_test)))\n",
        "#precision\n",
        "train_precision = precision_score(y_train, LR_model.predict(X_train_scaled), average=None)\n",
        "#test_precision = precision_score(y_test, LR_model.predict(scaler.transform(X_te#st)), average=None)\n",
        "# recall\n",
        "train_recall = recall_score(y_train, LR_model.predict(X_train_scaled), average=None)\n",
        "#test_recall = recall_score(y_test, LR_model.predict(scaler.transform(X_test)), average=None)\n",
        "# F1-score \n",
        "train_f1 = f1_score(y_train, LR_model.predict(X_train_scaled), average=None)\n",
        "#test_f1 = f1_score(y_test, LR_model.predict(scaler.transform(X_test)), average=None)\n",
        "\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irLR523pDKFI"
      },
      "outputs": [],
      "source": [
        "# Scale the test data\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXKVHVJjDPFc"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Compute the performance metrics on the test set\n",
        "\n",
        "# Compute the performance metrics on the training set\n",
        "#Compute the accuracy\n",
        "test_acc = accuracy_score(y_test, LR_model.predict(X_test_scaled))\n",
        "\n",
        "#precision\n",
        "test_precision = precision_score(y_test, LR_model.predict(X_test_scaled), average=None)\n",
        "# recall\n",
        "test_recall = recall_score(y_test, LR_model.predict(X_test_scaled), average=None)\n",
        "# F1-score \n",
        "test_f1 = f1_score(y_test, LR_model.predict(X_test_scaled), average=None)\n",
        "\n",
        "#How well does your model perform on each class of patients: those who have hypertension and those who do not\n",
        "\n",
        "metric_dic={'Train': [train_acc, train_precision, train_recall, train_f1], 'Test': [test_acc, test_precision, test_recall, test_f1]}\n",
        "metric_dic = pd.DataFrame(metric_dic, index=['Accuracy', 'Precision', 'Recall', 'F1-score']).T\n",
        "display(metric_dic)\n",
        "#------YOUR CODE HERE--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYzN-ef390B7"
      },
      "source": [
        "(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fUBiL55gJjLT"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'UMAP' from 'umap' (/Users/liyanran/opt/anaconda3/lib/python3.8/site-packages/umap/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d4c3909f4917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mumap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'UMAP' from 'umap' (/Users/liyanran/opt/anaconda3/lib/python3.8/site-packages/umap/__init__.py)"
          ]
        }
      ],
      "source": [
        "from umap import UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6XaGQBlJlsg"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Use UMAP to project the scaled training data onto two dimensions\n",
        "# Make sure to use random_state=456\n",
        "n_neighbors = 5\n",
        "min_dist = .3\n",
        "UMAP_model = UMAP(random_state=456, n_neighbors=n_neighbors, min_dist=min_dist)\n",
        "train_embed = UMAP_model.fit_transform(X_train_scaled)\n",
        "train_embed.shape\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xM4BPqFGLqz1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_embed' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-aaa12249bed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the UMAP embeddings on a scatter plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Spectral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'No Hypertension'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hypertension'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_embed' is not defined"
          ]
        }
      ],
      "source": [
        "from cProfile import label\n",
        "import matplotlib.pyplot as plt\n",
        "#------YOUR CODE HERE--------\n",
        "# Plot the UMAP embeddings on a scatter plot\n",
        "\n",
        "plt.scatter(train_embed[:,0], train_embed[:,1], c=y_train, cmap='Spectral', s=0.1)\n",
        "label = ['No Hypertension', 'Hypertension']\n",
        "plt.legend((0,1),label)\n",
        "\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnG1IehU-uQ5"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Use UMAP to project the scaled test data onto two dimensions\n",
        "# Make sure to use random_state=456\n",
        "\n",
        "\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S34GqU9Y-uQ6"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Plot the UMAP embeddings on a scatter plot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plsPZnjI_IOI"
      },
      "source": [
        "(i) Answer in your report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg0jl8JE_QKC"
      },
      "source": [
        "(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yCgQWLHrQd1K"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzUPm8HDRPfI"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Use LDA to project the scaled training data onto a single dimension\n",
        "\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "train_lda = lda_model.fit(X_train_scaled, y_train).transform(X_train_scaled).flatten()\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NFllfE7RxfL"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Plot the LDA embeddings on two histograms on the same plot\n",
        "no_hyper = train_lda[np.where(np.array(y_train) == 0)]\n",
        "hyper = train_lda[np.where(np.array(y_train) == 1)]\n",
        "\n",
        "plt.hist(no_hyper, bins=100, alpha=0.5, label='No Hypertension')\n",
        "plt.hist(hyper, bins=100, alpha=0.5, label='Hypertension')\n",
        "plt.xlabel('LDA Embedding')\n",
        "plt.ylabel('Count')\n",
        "plt.title('LDA Embeddings')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG4OWQEf_hXT"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Use LDA to project the scaled test data onto a single dimension\n",
        "\n",
        "test_lda = lda_model.fit(X_test_scaled, y_test).transform(X_test_scaled).flatten()\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBOb16cu_hXT"
      },
      "outputs": [],
      "source": [
        "#------YOUR CODE HERE--------\n",
        "# Plot the LDA embeddings on two histograms on the same plot\n",
        "\n",
        "no_hyper = test_lda[np.where(np.array(y_test) == 0)]\n",
        "hyper = test_lda[np.where(np.array(y_test) == 1)]\n",
        "\n",
        "plt.hist(no_hyper, bins=100, alpha=0.5, label='No Hypertension')\n",
        "plt.hist(hyper, bins=100, alpha=0.5, label='Hypertension')\n",
        "plt.xlabel('LDA Embedding')\n",
        "plt.ylabel('Count')\n",
        "plt.title('LDA Embeddings')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#------YOUR CODE ENDS--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D6ON4n6_w9K"
      },
      "source": [
        "(k) Answer in your report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtjpRNZD_y8N"
      },
      "source": [
        "(l) Answer in your report"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "816c00cb99d28fa964dcb8a0ec0763ff6c9749e85b2c40862bb8651e31a9cffc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
